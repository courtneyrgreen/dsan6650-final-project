---
title: "Conclusion and Future Work"
format: html
bibliography: references.bib
---

The results underscore a fundamental principle in reinforcement learning (RL) for operational challenges: an agent's actions are influenced by the rewards it receives, rather than the criteria by which humans ultimately assess success. The Newsvendor heuristic makes the most money in the Tiny Bakery environment because it uses a strategy that the RL reward doesn't like: making a lot of food, keeping inventory buffers, and accepting waste as a cost of keeping service levels high.

PPO, on the other hand, optimizes a composite reward that gives a lot of weight to wait times, abandonment, and leftover inventory. In order to reach that goal, the agent acts exactly as expected: it uses a conservative baking policy that limits exposure to negative reward terms but lowers throughput, service quality, and, in the end, profit. When trained on the same composite reward, the pricing bandit reinforces this pattern. Even the profit-only version can only change behavior a little bit because pricing can't fix the underlying capacity problems in ovens and queue dynamics.

These results are similar to what has been found in recent studies on RL for perishable goods, service systems, and dynamic pricing [@deepRLforDynamicPricing; @5bc07a4907c842fe8910904e812a0b1f; @genalti2021dynamicpricing]. In these studies, agents often look "suboptimal" on business metrics but are perfectly optimal under the shaped reward. They also fit with what reward design theory has said for a long time: if the shaped reward doesn't keep the best policy under transformation, even well-meaning punishments can lead to behavior that isn't economically desirable [@ng1999policy]. In this instance, the reward effectively instructs agents to perceive overproduction as more perilous than lost sales.

The experiments do not point out algorithmic failure; instead, they bring up the bigger problem of "objective misalignment," which is a well-known problem in the RL-for-operations literature and a major focus of safe reinforcement learning research [@garcia2015safe].

## Future Directions

There are a few extensions that could help close the alignment gap and make learning in the bakery environment more dynamic. One option is to change the rewards and set limits on the goals. A reward system that is based on profit or a limited RL formulation, like profit maximization with service-level penalties, could change behavior in a way that makes policies more economically sound. Techniques such as Lagrangian relaxation, CVaR-based learning, or multi-objective shaping provide more principled methods for encoding trade-offs among waste, service quality, and inventory risk. These methods are in line with the goals of risk-aware or safe reinforcement learning, which tries to find a balance between expected returns and downside control [@garcia2015safe].

Another direction is **training the curriculum and the environment**. It might be easier for the agent to get out of conservative local optima if they start with stationary demand and then move on to non-stationarity. This is similar to successful methods used in earlier research on pricing and inventory, where agents learn to generalize better when they are trained in steps.

It may also be very important to come up with "capacity-aware strategies." Short-horizon lookahead, bake-batch optimization, or different oven configurations are some of the new tools that the agent could use to deal with starvation regimes. In the same way, allowing limited restocking or soft limits on par levels could make the policy space bigger without making things less clear.

Another area of research could be **better baselines and hybrid methods**. More expressive par heuristics, contextual bandits for pricing, or RL–heuristic hybrids could better combine what we already know about a domain with what we learn. Newsvendor doesn't work because it's flexible; it works because its structural assumptions fit the problem well. Incorporating those assumptions into learning frameworks could produce more resilient, adaptable solutions.

Finally, the role of **alternative RL algorithms** deserves closer examination. Value-based or distributional methods, like QRDQN or variants of soft actor-critic (SAC), may interact differently with the same reward function—especially when long-tailed return distributions matter. As QRDQN experiments complete, a comparative analysis will help disentangle whether the conservative bias is due to the algorithm itself or to the underlying objective. Algorithms that clearly show uncertainty may be better at balancing risk and throughput in the long run.

In summary, these directions make it clear that the main problem is not adjusting PPO or adding more layers to the architecture. It is creating reward signals that convey operational objectives, such as profitability and service quality, in a manner that the agent can effectively optimize. The experiments presented herein provide a definitive diagnosis of the misalignment and a strategic framework for developing reinforcement learning systems that more accurately embody real-world business priorities.